{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing_Functions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "#string.punctuation\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import spacy\n",
        "nlp=spacy.blank(\"en\")\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "TMOcp0QskQcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6666a7c1-8561-4b08-ebca-73de4273d2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD9lKxPNhkzh"
      },
      "outputs": [],
      "source": [
        "text=\"Hello!!.... I am Akshata. i am going to MEET you tomorrow at 5 pm.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "  processed_text=\"\".join([i for i in text if i not in string.punctuation])\n",
        "  return processed_text\n",
        "text= remove_punctuation(text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Kx4uyyYejW3O",
        "outputId": "36a4c1ae-37ab-42b9-f93c-963aa474be1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello I am Akshata i am going to MEET you tomorrow at 5 pm'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lower(text):\n",
        "  processed_text=text.lower()\n",
        "  return processed_text\n",
        "text1=lower(text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VpQeQCJCkl5X",
        "outputId": "339b3202-0b52-4cf1-8632-3c6043be4382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello I am Akshata i am going to MEET you tomorrow at 5 pm'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(text):\n",
        "  processed_text=re.split('\\W+',text)\n",
        "  return processed_text\n",
        "text=tokenization(text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA-FFgRClP6x",
        "outputId": "7bd6c62a-0c84-4eec-bdf0-007a465378ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'I',\n",
              " 'am',\n",
              " 'Akshata',\n",
              " 'i',\n",
              " 'am',\n",
              " 'going',\n",
              " 'to',\n",
              " 'MEET',\n",
              " 'you',\n",
              " 'tomorrow',\n",
              " 'at',\n",
              " '5',\n",
              " 'pm']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "  processed_text=[i for i in text if i not in stopwords]\n",
        "  return processed_text\n",
        "text=remove_stopwords(text)  \n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an-uNag3mrPF",
        "outputId": "1f075605-88f1-4b6b-9d2d-573987d0daae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'I', 'Akshata', 'going', 'MEET', 'tomorrow', '5', 'pm']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(text):\n",
        "  processed_text=[WordNetLemmatizer().lemmatize(word)for word in text]\n",
        "  return processed_text\n",
        "text=lemmatization(text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMojYIAUo-xC",
        "outputId": "e5c0a008-f565-4702-8f23-b66cca0bc4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'I', 'Akshata', 'going', 'MEET', 'tomorrow', '5', 'pm']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text):\n",
        "    processed_text = [PorterStemmer().stem(word) for word in text]\n",
        "    return processed_text\n",
        "text=stemming(text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NDNYZrcr48p",
        "outputId": "fd6692f1-97d5-43d8-8907-45bdd5137117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'I', 'akshata', 'go', 'meet', 'tomorrow', '5', 'pm']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(\"they are eating pizza at 5 pm\")\n",
        "def alpha(doc):\n",
        "  return [token.is_alpha for token in doc]\n",
        "alpha(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx5ro-GcsxRs",
        "outputId": "28cafdb4-71ae-433a-aecd-c7c15a3fbdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True, True, True, True, True, False, True]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num(doc):\n",
        "  return [token.like_num for token in doc]\n",
        "num(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlziY1YJuuuv",
        "outputId": "7b796e67-90fc-40b7-d5b0-128f3f304f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False, False, False, False, False, True, False]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos(doc):\n",
        "  for token in doc:\n",
        "    print([token.text,token.pos_,token.dep_])\n",
        "pos(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxzoqOBmx0SG",
        "outputId": "ae963ba6-b50c-47ed-d9aa-129437e8d62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['they', 'PRON', 'nsubj']\n",
            "['are', 'AUX', 'aux']\n",
            "['eating', 'VERB', 'ROOT']\n",
            "['pizza', 'NOUN', 'dobj']\n",
            "['at', 'ADP', 'prep']\n",
            "['5', 'NUM', 'nummod']\n",
            "['pm', 'NOUN', 'pobj']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def entity(doc):\n",
        "    for ent in doc.ents:\n",
        "      print(ent.text, ent.label_)\n",
        "entity(doc)"
      ],
      "metadata": {
        "id": "dV3NwjTfyWpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0007289e-1764-4a78-cfc3-f3f9809ad158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 pm TIME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def similar_score(doc):\n",
        "  doc1=nlp(\"she is eating burger\")\n",
        "  print(doc1.similarity(doc))\n",
        "similar_score(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmaJ3-EpWorA",
        "outputId": "1e553428-a9a4-420d-c857-52f063453af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5184566655845401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    }
  ]
}